{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/meeami/anaconda3/envs/text-generation-inference/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ctranslate2 in /home/meeami/anaconda3/envs/text-generation-inference/lib/python3.9/site-packages (3.16.1)\n",
      "Requirement already satisfied: numpy in /home/meeami/anaconda3/envs/text-generation-inference/lib/python3.9/site-packages (from ctranslate2) (1.23.5)\n",
      "Requirement already satisfied: pyyaml<7,>=5.3 in /home/meeami/anaconda3/envs/text-generation-inference/lib/python3.9/site-packages (from ctranslate2) (6.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/meeami/anaconda3/envs/text-generation-inference/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/meeami/anaconda3/envs/text-generation-inference/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: py3nvml in /home/meeami/anaconda3/envs/text-generation-inference/lib/python3.9/site-packages (0.2.7)\n",
      "Requirement already satisfied: xmltodict in /home/meeami/anaconda3/envs/text-generation-inference/lib/python3.9/site-packages (from py3nvml) (0.13.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/meeami/anaconda3/envs/text-generation-inference/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ctranslate2\n",
    "!pip install py3nvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meeami/anaconda3/envs/text-generation-inference/lib/python3.9/site-packages/transformers/benchmark/benchmark_args_utils.py:136: FutureWarning: The class <class 'transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments'> is deprecated. Hugging Face Benchmarking utils are deprecated in general and it is advised to use external Benchmarking libraries  to benchmark Transformer models.\n",
      "  warnings.warn(\n",
      "/home/meeami/anaconda3/envs/text-generation-inference/lib/python3.9/site-packages/transformers/benchmark/benchmark_utils.py:615: FutureWarning: The class <class 'transformers.benchmark.benchmark.PyTorchBenchmark'> is deprecated. Hugging Face Benchmarking utils are deprecated in general and it is advised to use external Benchmarking libraries  to benchmark Transformer models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import PyTorchBenchmark, PyTorchBenchmarkArguments\n",
    "\n",
    "args = PyTorchBenchmarkArguments(models=[\"google/flan-t5-large\"], batch_sizes=[8], sequence_lengths=[8, 32, 128, 512])\n",
    "benchmark = PyTorchBenchmark(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 1\n",
      "\n",
      "====================       INFERENCE - SPEED - RESULT       ====================\n",
      "--------------------------------------------------------------------------------\n",
      "          Model Name             Batch Size     Seq Length     Time in s   \n",
      "--------------------------------------------------------------------------------\n",
      "     google/flan-t5-large            8               8             0.033     \n",
      "     google/flan-t5-large            8               32            0.078     \n",
      "     google/flan-t5-large            8              128            0.252     \n",
      "     google/flan-t5-large            8              512            1.188     \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "====================      INFERENCE - MEMORY - RESULT       ====================\n",
      "--------------------------------------------------------------------------------\n",
      "          Model Name             Batch Size     Seq Length    Memory in MB \n",
      "--------------------------------------------------------------------------------\n",
      "     google/flan-t5-large            8               8              4034     \n",
      "     google/flan-t5-large            8               32             4144     \n",
      "     google/flan-t5-large            8              128             4626     \n",
      "     google/flan-t5-large            8              512             6702     \n",
      "--------------------------------------------------------------------------------\n",
      "BenchmarkOutput(time_inference_result={'google/flan-t5-large': {'bs': [8], 'ss': [8, 32, 128, 512], 'result': {8: {8: 0.03313568998128176, 32: 0.07761786803603173, 128: 0.25215063337236643, 512: 1.188214035052806}}}}, memory_inference_result={'google/flan-t5-large': {'bs': [8], 'ss': [8, 32, 128, 512], 'result': {8: {8: 4034, 32: 4144, 128: 4626, 512: 6702}}}}, time_train_result={'google/flan-t5-large': {'bs': [8], 'ss': [8, 32, 128, 512], 'result': {8: {}}}}, memory_train_result={'google/flan-t5-large': {'bs': [8], 'ss': [8, 32, 128, 512], 'result': {8: {}}}}, inference_summary=None, train_summary=None)\n"
     ]
    }
   ],
   "source": [
    "results = benchmark.run()\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlanT5-Large (original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Wie alte sind Sie?</s>\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").cpu()\n",
    "\n",
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:35<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len:  8 time:  0.9559398126602173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:41<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len:  32 time:  1.6130975890159607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:54<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len:  128 time:  1.743881630897522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 100/100 [04:30<00:00,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len:  512 time:  2.7074382948875426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "seq_len = [8, 32, 128, 512]\n",
    "for i in range(4):\n",
    "    start = time.time()\n",
    "    for j in tqdm.tqdm(range(100)):\n",
    "        input_text = \"translate English to German: \" + \"house \"* seq_len[i]\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "        outputs = model.generate(input_ids)    \n",
    "        results = tokenizer.decode(outputs[0]) \n",
    "    end = time.time()\n",
    "    print(\"seq_len: \", seq_len[i], \"time: \", (end - start)/100)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLanT5-large ctranslate2(CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/meeami/anaconda3/envs/text-generation-inference/bin/ct2-transformers-converter\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/meeami/anaconda3/envs/text-generation-inference/lib/python3.9/site-packages/ctranslate2/converters/transformers.py\", line 1595, in main\n",
      "    converter.convert_from_args(args)\n",
      "  File \"/home/meeami/anaconda3/envs/text-generation-inference/lib/python3.9/site-packages/ctranslate2/converters/converter.py\", line 50, in convert_from_args\n",
      "    return self.convert(\n",
      "  File \"/home/meeami/anaconda3/envs/text-generation-inference/lib/python3.9/site-packages/ctranslate2/converters/converter.py\", line 84, in convert\n",
      "    raise RuntimeError(\n",
      "RuntimeError: output directory google/flan-t5-large-ct2 already exists, use --force to override\n"
     ]
    }
   ],
   "source": [
    "!ct2-transformers-converter --model google/flan-t5-large --output_dir google/flan-t5-large-ct2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meeami/anaconda3/envs/text-generation-inference/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-07-07 02:31:47.581320: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Haus ist wunderbar.\n"
     ]
    }
   ],
   "source": [
    "import ctranslate2\n",
    "import transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\") \n",
    "translator = ctranslate2.Translator(\"google/flan-t5-large-ct2\",device=\"cpu\")\n",
    "#print(translator.intra_threads,translator.inter_threads )\n",
    "\n",
    "\n",
    "input_text = \"translate English to German: The house is wonderful.\"\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(input_text))\n",
    "\n",
    "results = translator.translate_batch([input_tokens])\n",
    "\n",
    "output_tokens = results[0].hypotheses[0]\n",
    "output_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(output_tokens))\n",
    "\n",
    "print(output_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:03<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len:  8 time:  0.6320943832397461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:46<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len:  32 time:  1.6678615713119507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [01:54<25:31, 16.47s/it]"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import time\n",
    "seq_len = [8, 32, 128, 512]\n",
    "for i in range(4):\n",
    "    start = time.time()\n",
    "    for j in tqdm.tqdm(range(100)):\n",
    "        input_text =input_text = \"translate English to German: \" + \"house \"* seq_len[i]\n",
    "        input_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(input_text))\n",
    "        results = translator.translate_batch([input_tokens])\n",
    "        output_tokens = results[0].hypotheses[0]\n",
    "        output_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(output_tokens))\n",
    "    end = time.time()\n",
    "    print(\"seq_len: \", seq_len[i], \"time: \", (end - start)/100)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLanT5-large ctranslate2(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meeami/anaconda3/envs/text-generation-inference/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-07-07 02:37:58.338289: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Haus ist wunderbar.\n"
     ]
    }
   ],
   "source": [
    "import ctranslate2\n",
    "import transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\") \n",
    "translator = ctranslate2.Translator(\"google/flan-t5-large-ct2\",device=\"cuda\")\n",
    "\n",
    "\n",
    "input_text = \"translate English to German: The house is wonderful.\"\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(input_text))\n",
    "\n",
    "results = translator.translate_batch([input_tokens])\n",
    "\n",
    "output_tokens = results[0].hypotheses[0]\n",
    "output_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(output_tokens))\n",
    "\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 14.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len:  8 time:  0.06823009014129638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:21<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len:  32 time:  0.21241705179214476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:31<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len:  128 time:  2.1185588908195494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 100/100 [03:46<00:00,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len:  512 time:  2.2636704897880553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import time\n",
    "seq_len = [8, 32, 128, 512]\n",
    "for i in range(4):\n",
    "    start = time.time()\n",
    "    for j in tqdm.tqdm(range(100)):\n",
    "        input_text =input_text = \"translate English to German: \" + \"house \"* seq_len[i]\n",
    "        input_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(input_text))\n",
    "        results = translator.translate_batch([input_tokens])\n",
    "        output_tokens = results[0].hypotheses[0]\n",
    "        output_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(output_tokens))\n",
    "    end = time.time()\n",
    "    print(\"seq_len: \", seq_len[i], \"time: \", (end - start)/100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-generation-inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
